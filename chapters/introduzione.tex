%!TEX TS-program = pdflatex
%!TEX root = tesi.tex
%!TEX encoding = UTF-8 Unicode

\chapter{Introduzione}
\todo[inline]
{
  il probelma era tot
  l'obbiettivo era ottenere tot
  si è provato tot e tot
  risulta che gli autoencoder vanno bene
  cos'è un autoencoder
  come vengono usati di solito
}

\todo[inline]{TODO}
%In campo automotive, soprattutto negli ultimi anni, si è visto un crescente interesse nei confronti di sistemi di intelligenza artificiale che permettano di supervisionare la qualità dei pezzi prodotti.
%Assicurare la qualità è un requisito critico perché, in generale, la qualità influenza l'intera vita di un prodotto.
%Le applicazioni di algoritmi di Machine Learning prima, e di sistemi di Deep Learning poi, si sono dimostrate efficaci, flessibili e resilienti, portando numerosi vantaggi non solo nel campo del controllo automatico ma anche in quello del supporto agli operatori umani, ad esempio.
%I riferimenti alle applicazioni ben riuscite di sistemi intelligenti crescono di mese in mese ed offrono un ottimo mercato
%I riscontri 
%In un mercato 
%
%
%\todo[inline]{Spiegare cosa si intende con machine vision}
%
%\todo[inline]{Differenze machine vision vs. Computer vision}
%

% Machine vision
% % https://en.wikipedia.org/wiki/Machine_vision
% Computer vision
% % https://en.wikipedia.org/wiki/Computer_vision


% dire che librerie sono state usate
% dire che per noi True Positive vuole dire Scarto Scarto


\todo[inline]{START copiato da Autoencoder}

\clearpage
Facendo sempre rifermento a \cite{ng_sparse_ae}, un neurone è un'unità computazionale che prende in \textit{input} un vettore $x$ di elementi $x_i$ con $i=1,2,3,\dots,n$ e che ritorna $h_{W,b}(x) = f(W^Tx) = f(\sum_{i=1}^{n} W_i x_i + b)$.
Dove $h_{W,b}(x)$ è un generatore di ipotesi non-lineare, con parametri $W$ e $b$ che possono adattarsi al \textit{dataset}.
I pesi del neurone vengono salvati in $W$, vettore con tanti elementi quanti quelli di $x$.
%bias = pregiudizio
Il valore $b$ viene detto \textit{bias} è una quantità che verrà sempre sommata al risultato del prodotto riga per colonna tra $W$ e $x$.
% TODO fare figura a cui posso riferirmi per spiegare dove sono messi i pesi
La funzione $f$ è chiamata funzione di attivazione.
Come funzione di attivazione viene spesso utilizzata la funzione sigmoidea:
\begin{equation*}
  f(z) = \frac{1}{1 + exp(-z)}
\end{equation*} %TODO fare plot?
Notare come $f(z)$ sia una una funzione da $\mathbb{R}$ in $[0,1]$.
Ciò risulta utile soprattutto se il compito del neurone è dividere gli \textit{input} in due classi distinte, cioè $0$ e $1$, basterà verificare se $f(z)>0.5$.
In caso affermativo l'\textit{input} verrà associato alla classe con etichetta $1$, altrimenti a $0$;
Un'altra funzione di attivazione molto utilizzata è la funzione \textit{ReLU}
\todo[inline]{TODO descrivere relu}
\todo[inline]{TODO mostrare i grafici si sigmoid e relu?}


Quando uno strato della rete è composto da più neuroni, ciascuno di essi riceverà una copia di $x$ e ritornerà un \textit{output} in base ai propri $W$ e $b$.
%In figura~\ref{fig:semplice_ae} si può notare come gli $x_i$ vengano distribuiti su tutti i nodi del primo strato e come il loro risultato venga a sua volta distribuito sulle altre unità.
Dato che il numero di connessioni cresce rapidamente, essendo pari a $n*m$ per ogni strato (con $n$ la dimensione del vettore in ingresso ed $m$ di quello in uscita), questi strati vengono chiamati densamente connessi o, più semplicemente, densi.
% TODO qui parentesi sulla back prop?
\todo[inline]{qui parentesi sulla back prop?}
\todo[inline]{qui parentesi sulla (MSE)Loss?}
\todo[inline]{qui parentesi sul learning rate?}
\todo[inline]{qui parentesi sulle epoche?}

\clearpage
\section{Applicazioni principali}
Gli autoencoder non sono stati usati solamente come tecniche alternative per la \textit{dimensionality reduction}, seguono due delle principali applicazioni.

\paragraph{Denoising}
Con \textit{denoising} si intende rimuovere del rumore dai dati in \textit{input}.
All'AE viene richiesto, a partire da dati con del rumore


%\paragraph{Variational}

\paragraph{Anomaly Detection}


\todo[inline]{La nostra applicazione si trova a cavallo tra le due appena descritte}
\clearpage

\todo[inline]{END copiato da Autoencoder}
\clearpage
